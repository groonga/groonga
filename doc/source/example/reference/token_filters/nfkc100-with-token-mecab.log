Execution example::

  tokenize   'TokenMecab("use_reading", true)'   "私は林檎を食べます。"   --token_filters 'TokenFilterNFKC100("unify_kana", true)'
  # [
  #   [
  #     0, 
  #     1337566253.89858, 
  #     0.000355720520019531
  #   ], 
  #   [
  #     {
  #       "force_prefix_search": false, 
  #       "position": 0, 
  #       "force_prefix": false, 
  #       "value": "わたし"
  #     }, 
  #     {
  #       "force_prefix_search": false, 
  #       "position": 1, 
  #       "force_prefix": false, 
  #       "value": "は"
  #     }, 
  #     {
  #       "force_prefix_search": false, 
  #       "position": 2, 
  #       "force_prefix": false, 
  #       "value": "りんご"
  #     }, 
  #     {
  #       "force_prefix_search": false, 
  #       "position": 3, 
  #       "force_prefix": false, 
  #       "value": "を"
  #     }, 
  #     {
  #       "force_prefix_search": false, 
  #       "position": 4, 
  #       "force_prefix": false, 
  #       "value": "たべ"
  #     }, 
  #     {
  #       "force_prefix_search": false, 
  #       "position": 5, 
  #       "force_prefix": false, 
  #       "value": "ます"
  #     }, 
  #     {
  #       "force_prefix_search": false, 
  #       "position": 6, 
  #       "force_prefix": false, 
  #       "value": "。"
  #     }
  #   ]
  # ]
