Execution example::

  tokenize --tokenizer 'TokenNgram("n", 3)' --string "Hello World"
  # [
  #   [
  #     0,
  #     1556064951.790899,
  #     0.0002274513244628906
  #   ],
  #   [
  #     {
  #       "value": "Hel",
  #       "position": 0,
  #       "force_prefix": false,
  #       "force_prefix_search": false
  #     },
  #     {
  #       "value": "ell",
  #       "position": 1,
  #       "force_prefix": false,
  #       "force_prefix_search": false
  #     },
  #     {
  #       "value": "llo",
  #       "position": 2,
  #       "force_prefix": false,
  #       "force_prefix_search": false
  #     },
  #     {
  #       "value": "lo ",
  #       "position": 3,
  #       "force_prefix": false,
  #       "force_prefix_search": false
  #     },
  #     {
  #       "value": "o W",
  #       "position": 4,
  #       "force_prefix": false,
  #       "force_prefix_search": false
  #     },
  #     {
  #       "value": " Wo",
  #       "position": 5,
  #       "force_prefix": false,
  #       "force_prefix_search": false
  #     },
  #     {
  #       "value": "Wor",
  #       "position": 6,
  #       "force_prefix": false,
  #       "force_prefix_search": false
  #     },
  #     {
  #       "value": "orl",
  #       "position": 7,
  #       "force_prefix": false,
  #       "force_prefix_search": false
  #     },
  #     {
  #       "value": "rld",
  #       "position": 8,
  #       "force_prefix": false,
  #       "force_prefix_search": false
  #     },
  #     {
  #       "value": "ld",
  #       "position": 9,
  #       "force_prefix": false,
  #       "force_prefix_search": false
  #     },
  #     {
  #       "value": "d",
  #       "position": 10,
  #       "force_prefix": false,
  #       "force_prefix_search": false
  #     }
  #   ]
  # ]
