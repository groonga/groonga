# This is too slow with HTTP chunked.
#@require-interface stdio

table_create --name Users --flags TABLE_PAT_KEY --key_type ShortText
column_create --table Users --name immediate --type Bool

# Reuse occurs when more than 256 nodes are deleted.
#@disable-logging
#@generate-series 1 130 Users '{"_key" => "%d" % i, "immediate" => true}'
#@generate-series 1 130 Users '{"_key" => "User%012d" % i, "immediate" => false}'
#@enable-logging
delete Users --filter true

check Users
defrag
check Users

# Test that the node is not reused when added after defragmentation.
# `_id` is not reused and is loaded as expected.
# `_id` is not reused, but is generated from a new `_id` of 261.
load --table Users
[
{"_key":"XXXX000000000001", "immediate":false},
{"_key":"XXXX000000000002", "immediate":false},
{"_key":"X99", "immediate":true}
]
select Users --sort_keys _key

# `dump` should still be normal after defragmentation (after clearing garbage).
dump Users
