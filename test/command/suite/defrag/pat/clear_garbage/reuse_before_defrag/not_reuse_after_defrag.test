# This is too slow with HTTP chunked.
#@require-interface stdio

table_create --name Users --flags TABLE_PAT_KEY --key_type ShortText
column_create --table Users --name immediate --type Bool

# Reuse occurs when more than 256 nodes are deleted.
#@disable-logging
#@generate-series 1 260 Users '{"_key" => "%d" % i, "immediate" => true}'
#@generate-series 1 260 Users '{"_key" => "A-User%04d" % i, "immediate" => false}'
#@enable-logging
delete Users --filter true
# Add reuse nodes.
load --table Users
[
{"_key":"A-User9999", "immediate":false},
{"_key":"A99", "immediate":true}
]

check Users
defrag
check Users

# Test that the node is not reused when added after defragmentation.
# `_id` is not reused and is loaded as expected.
# `_id` is not reused, but is generated from a new `_id` of 266.
load --table Users
[
{"_key":"C-User0001", "immediate":false},
{"_key":"C-User0002", "immediate":false},
{"_key":"C-User99", "immediate":false},
{"_key":"C01", "immediate":true},
{"_key":"C02", "immediate":true},
{"_key":"CC01", "immediate":true}
]

select Users --sort_keys _key

# `dump` should still be normal after defragmentation (after clearing garbage).
dump Users
