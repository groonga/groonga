# This is too slow with HTTP chunked.
#@require-interface stdio

table_create --name Users --flags TABLE_PAT_KEY --key_type ShortText
column_create --table Users --name immediate --type Bool

# Since only 60 nodes were deleted, no reuse occurred.
# (Reuse occurs when more than 256 nodes are deleted.)
#@disable-logging
#@generate-series 1 30 Users '{"_key" => "%d" % i, "immediate" => true}'
#@generate-series 1 30 Users '{"_key" => "User%012d" % i, "immediate" => false}'
#@enable-logging
delete Users --filter true

# The garbage is cleared.
check Users
defrag
check Users

# Load succeeds after defragmentation.
load --table Users
[
{"_key":"XXXX000000000001", "immediate":false},
{"_key":"XXXX000000000002", "immediate":false},
{"_key":"X99", "immediate":true}
]
select Users --sort_keys _key

# `dump` should still be normal after defragmentation (after clearing garbage).
dump Users
